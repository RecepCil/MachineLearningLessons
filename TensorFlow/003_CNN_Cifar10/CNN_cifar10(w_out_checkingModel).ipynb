{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"CNN_cifar10(w/out_checkingModel).ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"metadata":{"id":"5BNMs-UyXVjE","colab_type":"code","colab":{}},"cell_type":"code","source":["import tensorflow as tf\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import time\n","from datetime import timedelta\n","\n","from google.colab import files\n","#src = list(files.upload().values())[0]    # add a new lib from computer\n","#open('mylib.py','wb').write(src)\n","import mylib\n","\n","import cifar10\n","\n","cifar10.download()\n","\n","print(cifar10.load_class_names())\n","\n","# get images, classes and labels of \"train & test\"\n","train_img, train_cls, train_labels = cifar10.load_training_data()\n","test_img, test_cls, test_labels = cifar10.load_test_data()\n","\n","# How many images are that we got\n","print(len(train_img), len(test_img))    #50k 10k\n","\n","x = tf.placeholder(tf.float32, [None, 32, 32, 3])\n","y_true = tf.placeholder(tf.float32, [None, 10])\n","pkeep = tf.placeholder(tf.float32)\n","phase = tf.placeholder(tf.bool)  #eğitim aşamasında mı? test aşamasında mı? tutucak\n","\n","###DATA AUGMENTATION###\n","#Data Augmentation\n","def pre_process_image(image):\n","  image = tf.image.random_flip_left_right(image) #resmi ters çevir\n","  image = tf.image.random_hue(image, max_delta=0.05) #resmin renkleriyle oyna\n","  image = tf.image.random_contrast(image, lower=0.3, upper=1.0)\n","  image = tf.image.random_brightness(image, max_delta=0.2)\n","  image = tf.image.random_saturation(image, lower=0.0, upper=2.0) #doygunlukla oyna\n","  \n","  image = tf.minimum(image, 1.0)  # güvenlik önlemi\n","  image = tf.maximum(image, 0.0)\n","  return image\n"," \n","#pre_process_image'ı tüm batch'e uygula \n","def pre_process(images):\n","  images = tf.map_fn(lambda image: pre_process_image(image), images)\n","  return images\n","  \n","with tf.device('/cpu:0'):      # bu kısmı, cpu'da çalıştır(performans amaçlı)\n","  distorted_images = pre_process(images=x)\n","### end of DATA AUGMENTATION###\n","  \n","  \n","###NORMALIZATION###\n","                                    #scope isimle alakalı\n","def batch_normalization(input, phase, scope):\n","  return tf.cond(phase,    #if phase=true(eğitim)\n","                 lambda: tf.contrib.layers.batch_norm(input, decay=0.99, is_training=True,\n","                                                     updates_collections=None, center=True, scope=scope),  #center=True ortalamasını sağlıyor\n","                 lambda: tf.contrib.layers.batch_norm(input, decay=0.99, is_training=False,\n","                                                     updates_collections=None, center=True, scope=scope, reuse=True))  #reuse: eğitilen w'leri tekrar kullanmak için\n","  \n","  \n","  \n","###end of NORMALIZATION###\n","  \n","  \n","#Convulasyon Layer \n","def conv_layer(input, size_in, size_out, scope, use_pooling=True):\n","                                  # 3x3 'lük filtre\n","  w = tf.Variable(tf.truncated_normal([3, 3, size_in, size_out], stddev=0.1))\n","  b = tf.Variable(tf.constant(0.1, shape=[size_out]))    #shape is size of bias\n","                               #atılımlar\n","  conv = tf.nn.conv2d(input, w, strides=[1, 1, 1, 1], padding='SAME') + b\n","  conv_bn = batch_normalization(conv, phase, scope)   #Normalization\n","  y = tf.nn.relu(conv_bn)    #relu, aktivasyon fonksiyonumuz\n","  \n","  if use_pooling:         #boyut 2x2 olduğundan atılım da 2x2 ki çakışma olmasın\n","    y = tf.nn.max_pool(y, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n","    \n","  return y\n","\n","\n","#fully connected layer\n","def fc_layer(input, size_in, size_out, scope, relu=True,dropout=True, batch_norm=False):\n","  w = tf.Variable(tf.truncated_normal([size_in, size_out], stddev=0.1))\n","  b = tf.Variable(tf.constant(0.1, shape=[size_out]))\n","  logits = tf.matmul(input, w) + b\n","  \n","  if batch_norm:    #Normalization\n","    logits = batch_normalization(logits, phase, scope)\n","  \n","  if relu:\n","    y = tf.nn.relu(logits)\n","    if dropout:\n","      y = tf.nn.dropout(y, pkeep)\n","    return y\n","  else:\n","    return logits\n","  \n","# input, derinlik(veya kanal sayısı), kaç filtre olucak\n","conv1 = conv_layer(distorted_images, 3, 32, scope='conv1', use_pooling=True)   #işlem bittiğinde 16,16,32 olucak     scope='conv1', layer'ın ismini atadık\n","conv2 = conv_layer(conv1, 32, 64, scope='conv2', use_pooling=True)     #işlem bittiğinde 8,8,64 olucak\n","conv3 = conv_layer(conv2, 64, 64, scope='conv3', use_pooling=True)     #işlem bittiğinde 4,4,64 olucak\n","\n","# conv layer'ı, fully connected layer'a bağlaman için düzleştirmen lazım\n","flattened = tf.reshape(conv3, [-1, 4*4*64])\n","\n","fc1 = fc_layer(flattened, 4*4*64, 512, scope='fc1', relu=True, dropout=True, batch_norm=True)\n","fc2 = fc_layer(fc1, 512, 256, scope='fc2', relu=True, dropout=True, batch_norm=True)\n","#output layer\n","#fc3 = logits\n","logits = fc_layer(fc2, 256, 10, scope='fc_out', relu=False, dropout=False, batch_norm=False)\n","y = tf.nn.softmax(logits) # bu kez relu yerine softmax kullandık\n","\n","y_pred_cls = tf.argmax(y, 1)   # tahmin edilen sınıfı tut (1=dimension)\n","\n","xent = tf.nn.softmax_cross_entropy_with_logits(logits=logits,labels=y_true)\n","loss = tf.reduce_mean(xent)\n","\n","correct_prediction = tf.equal(y_pred_cls, tf.argmax(y_true, 1))\n","accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n","\n","optimizer = tf.train.AdamOptimizer(5e-4).minimize(loss)\n","\n","sess = tf.Session()\n","sess.run(tf.global_variables_initializer())\n","\n","# eğitim esnasında hafıza hatası alırsan burayı düşür\n","batch_size = 128\n","\n","def random_batch():\n","  # rastgele bir index oluşturuyorum, bu index'teki verileri al\n","  index = np.random.choice(len(train_img),size=batch_size, replace=False)\n","  x_batch = train_img[index, :, :, :]\n","  y_batch = train_labels[index, :]\n","  \n","  return x_batch, y_batch\n","  \n","loss_graph = []\n","\n","def training_step (iterations):\n","  start_time = time.time()   #fonk başladığı andaki zamanı al\n","  for i in range (iterations):\n","    x_batch, y_batch = random_batch()\n","    feed_dict_train = {x: x_batch, y_true: y_batch, pkeep: 0.5, phase: True}\n","    [_, train_loss] = sess.run([optimizer, loss], feed_dict=feed_dict_train)\n","    loss_graph.append(train_loss)\n","  \n","    if i % 100 == 0 :  \n","      acc = sess.run(accuracy, feed_dict = feed_dict_train)\n","      print('Iteration:', i, 'Training accuracy:', acc, 'Training loss:', train_loss)\n","  \n","  end_time = time.time()\n","  time_dif = end_time - start_time\n","  print(\"Time usage:\", timedelta(seconds=int(round(time_dif))))\n","  \n","#test\n","batch_size_test = 256\n","\n","def test_accuracy():\n","  num_images = len(test_img)\n","  cls_pred = np.zeros(shape=num_images, dtype = np.int)\n","  i=0\n","  \n","  while i < num_images:\n","    j = min(i+batch_size_test,num_images) #num_images'i geçmesin diye\n","    feed_dict = { x: test_img[i:j, :], y_true: test_labels[i:j, :], pkeep: 1, phase: False}\n","    cls_pred[i:j] = sess.run(y_pred_cls, feed_dict=feed_dict)\n","    i = j #i'yi batch_size_test kadar arttırıp, b_s_t kadar veri alıyoruz\n","    \n","  correct = (test_cls == cls_pred)\n","  print('Test accuracy:',correct.mean())\n","  \n","  \n","  \n","\n","training_step(10000)\n","test_accuracy()\n","\n","'''\n","plt.plot(loss_graph,'k-')\n","plt.title('Loss Grafiği')\n","plt.xlabel('Iteration')\n","plt.ylabel('Loss')\n","plt.show()\n","'''\n","  \n","  \n","  \n","  \n","  \n","  "],"execution_count":0,"outputs":[]}]}